{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "project8_logisticregression_pca_breastcancer_Wisconsin.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyKBi6ABn9Au",
        "colab_type": "text"
      },
      "source": [
        "# Project objective\n",
        "This project is designed to review PCA as a dimensionality reduction approach in combination with logistic regression as a supervised machine learning method and their implementation in python using Olivetti faces data-set from AT&T dataset.\n",
        "\n",
        "Information about the dataset, some technical details about the used machine learning method(s) and mathematical details of the quantifications approaches are provided in the code. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjtJFxdsNh05",
        "colab_type": "text"
      },
      "source": [
        "# Packages we work with in this notebook\n",
        "We are going to use the following libraries and packages:\n",
        "\n",
        "* **numpy**: NumPy is the fundamental package for scientific computing with Python. (http://www.numpy.org/)\n",
        "* **sklearn**: Scikit-learn is a machine learning library for Python programming language. (https://scikit-learn.org/stable/)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57oB2idEgr-g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import sklearn as sk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bb1Zm7ARN5D5",
        "colab_type": "text"
      },
      "source": [
        "# Introduction to the dataset\n",
        "\n",
        "**Name**: Olivetti faces data-set from AT&T dataset\n",
        "\n",
        "**Summary**: This dataset consists of 10 pictures each of 40 individuals.\n",
        "\n",
        "**number of features**: 4096 (real, positive) \n",
        "\n",
        "**Number of data points (instances)**: 400\n",
        "\n",
        "**dataset accessibility**: Dataset is available as part of sklearn package.\n",
        "\n",
        "**Link to the dataset**: http://lijiancheng0614.github.io/scikit-learn/datasets/olivetti_faces.html\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjBnejgpP0Gr",
        "colab_type": "text"
      },
      "source": [
        "## Loading the dataset and separating features and labels\n",
        "The dataset is available as part of sklearn package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RILQWrhjQUtF",
        "colab_type": "code",
        "outputId": "1991d64e-cfb9-4e54-b824-18d4e30497cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "from sklearn.datasets import fetch_olivetti_faces\n",
        "\n",
        "# Loading breast cancer data\n",
        "target_dataset = fetch_olivetti_faces()\n",
        "\n",
        "# separating feature arrays of pixel values (X) and labels (y) \n",
        "input_features = target_dataset.data\n",
        "output_var = target_dataset.target\n",
        "# printing number of features (pixels) and data points \n",
        "n_samples, n_features = input_features.shape\n",
        "print(\"number of samples (data points):\", n_samples)\n",
        "print(\"number of features:\", n_features)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of samples (data points): 400\n",
            "number of features: 4096\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgejl_XWhWqN",
        "colab_type": "text"
      },
      "source": [
        "## Splitting data to training and testing sets\n",
        "\n",
        "We need to split the data to train and test, if we do not have a separate dataset for validation and/or testing, to make sure about generalizability of the model we train.\n",
        "\n",
        "**test_size**: Traditionally, 30%-40% of the dataset cna be used for test set. If you split the data to train, validation and test, you can use 60%, 20% and 20% of teh dataset, respectively.\n",
        "\n",
        "**Note.**: We need the validation and test sets to be big enough for checking generalizability of our model. At the same time we would like to have as much data as possible in the training set to train a better model.\n",
        "\n",
        "**random_state** as the name suggests, is used for initializing the internal random number generator, which will decide the splitting of data into train and test indices in your case.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3L9BbkSg2vp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(input_features, output_var, test_size=0.30, random_state=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzmyMR3VpV-_",
        "colab_type": "text"
      },
      "source": [
        "## Feature extraction (unsupervised)\n",
        "We want to implement principle component analysis (PCA) to combine features. Considering the dataset in this code, there are too many features compared to the number of data points. Hence, this feature combination that reduced dimensionality of the dataset could help imporoving performance of supervised learning model we want to develop later in this code.\n",
        "\n",
        "### Principle component analysis (PCA)\n",
        "Principal component analysis creates new orthogonal variables (principle components) that are linear combinations of the original variables. The focus of PCA is to reproduce the total variance in the original higher dimensional space in the lower dimensional space.\n",
        "PCA is an optimum approach for mapping to the lower dimensional space and be able to reconstruct the original space afterward.\n",
        "\n",
        "1) The first principal component (PC) corresponds to a line that passes through the mean. The lines is the regression line so that it minimizes the sum of squares of the distances of the points from the line. \n",
        "\n",
        "2) The second PC corresponds to the same concept after all correlation with the first principal component has been subtracted from the points.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fm8Jlti8qSHD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import decomposition\n",
        "\n",
        "# we want to reduce dimensionality of the dataset to 150 (150 is arbitrary in this code)\n",
        "\n",
        "# Create PCA object\n",
        "pca = decomposition.PCA(n_components=150,whiten=True, random_state = 42)\n",
        "\n",
        "# fitting the PCA model using the training data\n",
        "pca.fit_transform(X_train)\n",
        "# generate principle components of the training data\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "\n",
        "# let's now identify principle components in the test set using the mapping identified in the training set\n",
        "X_test_pca = pca.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0Fi1jpVkbhC",
        "colab_type": "text"
      },
      "source": [
        "## Building the supervised learning model\n",
        "We want to build a binary classification model as the output variable is categorical with 2 classes. Here we build a simple logistic regression model.\n",
        "\n",
        "### Logistic regression\n",
        "If we have set of features X1 to Xn, y can be obtained as:\n",
        "\\begin{equation*} y=b0+b1X1+b2X2+...+bnXn\\end{equation*}\n",
        "\n",
        "where y is the predicted value obtained by weighted sum of the feature values.\n",
        "\n",
        "Then probability of each class (for example if there is a malignant tumor) can be obtained using the logistic function \n",
        "\n",
        "\\begin{equation*} p(class=malignant)=\\frac{1}{(1+exp(-y))} \\end{equation*}\n",
        "\n",
        "Based on the given class labels and the features given in the trainign data, coefficients b0 to bn can be ontained during the optimization process.\n",
        "\n",
        "b0 to bn are fixed for all samples while X1 to Xn are feature values specific to each sample. Hence, the logistic function will give us probability of each class assigned to each sample. Finally, the model will choose the class with the highest probability for each sample.\n",
        "\n",
        "\n",
        "**Note.** The logistic regression model is parametric and the parameters are the regression coefficiets b0 to bn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fj3SSteMkxb2",
        "colab_type": "code",
        "outputId": "1283538e-4a36-4598-e3b9-d562274cf266",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        }
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression \n",
        "\n",
        "# Create logistic regression object\n",
        "logreg = LogisticRegression(random_state = 42)\n",
        "\n",
        "# Train the model using the training sets\n",
        "logreg.fit(X_train, y_train)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUWtodtDmFAW",
        "colab_type": "text"
      },
      "source": [
        "## Prediction of test (or validation) set\n",
        "We now have to use the trained model to predict y_test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRQpG2vykzqX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make predictions using the testing set\n",
        "y_pred = logreg.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cxrrmey9tVDZ",
        "colab_type": "text"
      },
      "source": [
        "## Implementing the supoervised learnign model on the reduced dimensions\n",
        "We now want to repeat the same process using the new features (principle components).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkWzLtAVtiUC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create logistic regression object\n",
        "logreg_pca = LogisticRegression(random_state = 42)\n",
        "\n",
        "# Train the model using the training sets\n",
        "logreg_pca.fit(X_train_pca, y_train)\n",
        "\n",
        "# Make predictions using the testing set\n",
        "y_pred_pca = logreg_pca.predict(X_test_pca)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUHXoiaemXai",
        "colab_type": "text"
      },
      "source": [
        "## Evaluating performance of the model\n",
        "We need to assess performance of the model using the predictions of the test set. We use accuracy and balanced accuracy. Here are their definitions:\n",
        "\n",
        "* **recall** in this context is also referred to as the true positive rate or sensitivity\n",
        "\n",
        "How many relevant item are selected\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "$${\\displaystyle {\\text{recall}}={\\frac {tp}{tp+fn}}\\,} $$\n",
        "\n",
        " \n",
        "\n",
        "* **specificity** true negative rate\n",
        "\n",
        "\n",
        "\n",
        "$${\\displaystyle {\\text{true negative rate}}={\\frac {tn}{tn+fp}}\\,}$$\n",
        "\n",
        "* **accuracy**: This measure gives you a sense of performance for all the classes together as follows:\n",
        "\n",
        "$$ {\\displaystyle {\\text{accuracy}}={\\frac {tp+tn}{tp+tn+fp+fn}}\\,}$$\n",
        "\n",
        "\n",
        "\\begin{equation*} accuracy=\\frac{number\\:of\\:correct\\:predictions}{(total\\:number\\:of\\:data\\:points (samples))} \\end{equation*}\n",
        "\n",
        "\n",
        "* **balanced accuracy**: This measure gives you a sense of performance for all the classes together as follows:\n",
        "\n",
        "$${\\displaystyle {\\text{balanced accuracy}}={\\frac {recall+specificity\n",
        "}{2}}\\,}$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAqcUM5Bmb2d",
        "colab_type": "code",
        "outputId": "0368e666-2ca2-44b1-b908-b93111ab840b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "print(\"accuracy of the predictions using original features:\", metrics.accuracy_score(y_test, y_pred))\n",
        "print(\"accuracy of the predictions using new features (principle components):\", metrics.accuracy_score(y_test, y_pred_pca))\n",
        "print(\"blanced accuracy of the predictions using original features:\", metrics.balanced_accuracy_score(y_test, y_pred))\n",
        "print(\"blanced accuracy of the predictions using new features (principle components):\", metrics.balanced_accuracy_score(y_test, y_pred_pca))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy of the predictions using original features: 0.95\n",
            "accuracy of the predictions using new features (principle components): 0.95\n",
            "blanced accuracy of the predictions using original features: 0.9566666666666667\n",
            "blanced accuracy of the predictions using new features (principle components): 0.96\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axk2l7cawgl5",
        "colab_type": "text"
      },
      "source": [
        "## Take-home message\n",
        "As we can see, perrormance of the logistic regression model using principle components is the same or a bit better (not significant) than the model with original features. However, the important point is that we could achieve this performance with only 150 features (principle components) instead of 4096 original features. Reducing number of dimensions (features) can help us to reduce memory usage and running time while at the saem time it can helop us to get rid of redundant and noisy features.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8d5eJVRm2yO",
        "colab_type": "text"
      },
      "source": [
        "## Extracting the coefficient of the model\n",
        "The trained logistic regresseion model predicts the class of a datapoint as a fucntion of linear combination of feature values. Hence, each feature has a coefficient in this linear combination for predicting output variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ELrsRLImqtM",
        "colab_type": "code",
        "outputId": "84112d80-c7d0-493e-85cb-d84355810b89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        }
      },
      "source": [
        "#print('Coefficients using original features: {}'.format(logreg.coef_))\n",
        "print('Coefficients using new features (principle components): {}'.format(logreg_pca.coef_))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Coefficients using new features (principle components): [[-0.26044098  0.10272042  0.10610443 ... -0.02761116 -0.01568002\n",
            "   0.02889717]\n",
            " [ 0.00231089 -0.15434158 -0.25431996 ... -0.01134162 -0.02918408\n",
            "  -0.06746296]\n",
            " [-0.01215432  0.00632089  0.2470995  ... -0.08977587 -0.03819557\n",
            "  -0.02374   ]\n",
            " ...\n",
            " [-0.02580804 -0.05658147  0.30839334 ...  0.06881294  0.0957967\n",
            "  -0.03936866]\n",
            " [ 0.55723037  0.09982581 -0.08493365 ... -0.01938    -0.03544625\n",
            "  -0.00536038]\n",
            " [-0.11796729  0.12682501  0.1481887  ... -0.05140645 -0.12856949\n",
            "   0.17752318]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}