{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "project18_CNN_MNIST.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0539Pwt1Ygwz",
        "colab_type": "text"
      },
      "source": [
        "# Project objective\n",
        "In this project, we build a convolutional neural network model for classification of images of hand-written digits in MNIST dataset. \n",
        "\n",
        "Information about the dataset, some technical details about the used machine learning method(s) and mathematical details of the quantifications approaches are provided in the code. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjtJFxdsNh05",
        "colab_type": "text"
      },
      "source": [
        "# Packages we work with in this notebook\n",
        "We are going to use the following libraries and packages:\n",
        "\n",
        "* **numpy**: NumPy is the fundamental package for scientific computing with Python. (http://www.numpy.org/)\n",
        "* **sklearn**: Scikit-learn is a machine learning library for Python programming language. (https://scikit-learn.org/stable/)\n",
        "* **keras**: keras is a widely-used neural network framework in python. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57oB2idEgr-g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import keras "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bb1Zm7ARN5D5",
        "colab_type": "text"
      },
      "source": [
        "# Introduction to the dataset\n",
        "\n",
        "**Name**: MNIST hand-writtent digits\n",
        "\n",
        "**Summary**: Recognizing hand-written digits  \n",
        "\n",
        "**number of features**: 28 pixels in rows and 28 pixels in columns \n",
        "\n",
        "**Number of data points (instances)**: 70,000 (60,000 for trainign and 10,000 for test set)\n",
        "\n",
        "**dataset accessibility**: Dataset is available as part of keras (https://keras.io/api/datasets/)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjBnejgpP0Gr",
        "colab_type": "text"
      },
      "source": [
        "## Importing the dataset and splitting the data training and testing sets\n",
        "We can easily load the data from keras. We need to then split the data to train and test, if we do not have a separate dataset for validation and/or testing, to make sure about generalizability of the model we train. However, the splitting also has been already taken care of in keras. So we can load the data as trainig and test sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RILQWrhjQUtF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "72c1c335-1c4b-4900-a26a-450e335754c2"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "#download mnist data and split into train and test sets\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qW4bVMDCdPVW",
        "colab_type": "text"
      },
      "source": [
        "## Making sure about the dataset characteristics (number of data points and features)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpSupKvgdS3e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c5256dea-329b-4d6b-f1dc-a4edeaab0ccb"
      },
      "source": [
        "print('Shapes of the images: {}'.format(X_train[0].shape))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shapes of the images: (28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIX-LbyLeEc6",
        "colab_type": "text"
      },
      "source": [
        "## Data preparation\n",
        "We need to prepare the dataset for machine learnign modeling. Here we prepare the data in 2 steps:\n",
        "\n",
        "1) Reshaping input features (pixels of images) to the shape that we can later use in the modeling.\n",
        "\n",
        "2) Converting the integer array of labels to one-hot encodings to be used in neural network modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GI52MUkePCR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fb954ca2-b33b-4ffd-fbcf-1d21aba54edb"
      },
      "source": [
        "#reshape data to fit model\n",
        "# Interpretation of the numbers in the reshape function\n",
        "# 1) number of data points (images)\n",
        "# 2 and 3) number of pixels in rows and columns\n",
        "# 4) 1 stands for greyscale\n",
        "X_train = X_train.reshape(60000,28,28,1)\n",
        "X_test = X_test.reshape(10000,28,28,1)\n",
        "\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "#one-hot encode target column\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "print('One hot vector of the first image in the training set: {}'.format(y_train[0]))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "One hot vector of the first image in the training set: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0Fi1jpVkbhC",
        "colab_type": "text"
      },
      "source": [
        "## Building the supervised learning model\n",
        "We want to build a multi-class classification model as the output variable include multiple classes. \n",
        "\n",
        "\n",
        "Here we build a neural network model with 2 hidden layers. A neural network with 2 or more hidden layers are called deep neural network. So technical it is a deep learning code. As you can see the implementation of a deep learning model is not difficult. But knowing how to interpret it, how to fine-tune the model and avoid overfitting are the parts that need experience and more knowledge.\n",
        "\n",
        "\n",
        "### Convolutional neural network\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fj3SSteMkxb2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, Flatten\n",
        "# building a neural network model\n",
        "model = Sequential()\n",
        "\n",
        "# Adding 1st hidden layer with 32 as the number of output filters in the convolution\n",
        "# input_dim should be specified as the number of input features\n",
        "# kernel_size is the size of the convolutional filters. Here we are using 3*3 filteres.\n",
        "model.add(Conv2D(32, kernel_size=3, activation='relu', input_shape=(28,28,1)))\n",
        "\n",
        "# Adding 2nd hidden layer with 16 as the number of output filters in the convolution\n",
        "model.add(Conv2D(16, kernel_size=3, activation='relu'))\n",
        "\n",
        "# We want to flatten the image shape matrices of the last hidden layer\n",
        "# and then use the results in the output layer\n",
        "model.add(Flatten())\n",
        "\n",
        "# adding the output layer (softmax is used to generate probabilities for each predicted class)\n",
        "# Size if the last layer should be equal to the total number of classes in the dataset which is 10\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# compiling the model using cross-entropy for categorical variables,\n",
        "# as we are dealing with multi-class classification\n",
        "# Adam optimization algorithm is also used\n",
        "# Accuracy is used as the metric to assess performance of our model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgszolas0q7X",
        "colab_type": "text"
      },
      "source": [
        "Now we fit our neural network model using the training set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Od9xWVWhzp28",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "9a079f47-4fe3-4849-f184-a2654ee02ee7"
      },
      "source": [
        "# Train the model using the training set\n",
        "# We can also check the performance of the model after every epoch on the validation set. \n",
        "# Here we use the test set of MNIST dataset to check the performance on the validation set.\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 67s 35ms/step - loss: 0.2691 - accuracy: 0.9467 - val_loss: 0.1042 - val_accuracy: 0.9680\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 66s 35ms/step - loss: 0.0754 - accuracy: 0.9775 - val_loss: 0.0922 - val_accuracy: 0.9738\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 66s 35ms/step - loss: 0.0522 - accuracy: 0.9840 - val_loss: 0.0887 - val_accuracy: 0.9735\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 66s 35ms/step - loss: 0.0390 - accuracy: 0.9875 - val_loss: 0.0848 - val_accuracy: 0.9780\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 66s 35ms/step - loss: 0.0307 - accuracy: 0.9902 - val_loss: 0.1162 - val_accuracy: 0.9758\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 67s 36ms/step - loss: 0.0283 - accuracy: 0.9913 - val_loss: 0.1233 - val_accuracy: 0.9775\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 67s 35ms/step - loss: 0.0207 - accuracy: 0.9938 - val_loss: 0.1485 - val_accuracy: 0.9715\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 66s 35ms/step - loss: 0.0184 - accuracy: 0.9944 - val_loss: 0.1473 - val_accuracy: 0.9787\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 66s 35ms/step - loss: 0.0192 - accuracy: 0.9946 - val_loss: 0.1601 - val_accuracy: 0.9783\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 66s 35ms/step - loss: 0.0158 - accuracy: 0.9954 - val_loss: 0.1987 - val_accuracy: 0.9774\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f37abc207b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWb1aK68rGfT",
        "colab_type": "text"
      },
      "source": [
        "***Note***. Accuracy of the model in the validation set is higher than training set after the first epoch. This is not something unusual. The model could perform better in teh validation set after the first few epochs as the number and distribution of datapoints are different between the trainign and validation sets. However, as the model gets better and better, trainign set accuracy goes abovevalidation set accuracy.\n",
        "\n",
        "**Early stopping**: The concept of early stopping tells us that we can stop when the validation loss starts increasing in a consistent way, not small increases due to oscillatory behavior of the optimization process. Hence, stopping the model after 4 epochs of training for this model would have been a good choice.\n",
        "\n",
        "The model is trained now and can be used to predict the lables of datapoints in the test set. To be able to assess the performance of the predictions in the test set using metrics class in sklearn, we need to transform the true lables and the predictions from one-hot encodings to lists."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyBVyd9QqS4e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = model.predict(X_test)\n",
        "#Converting predictions to label\n",
        "pred = list()\n",
        "for i in range(len(y_pred)):\n",
        "    pred.append(np.argmax(y_pred[i]))\n",
        "#Converting one hot encoded test label to label\n",
        "test = list()\n",
        "for i in range(len(y_test)):\n",
        "    test.append(np.argmax(y_test[i]))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-Uby3AV1OID",
        "colab_type": "text"
      },
      "source": [
        "## Evaluating performance of the model\n",
        "We need to assess performance of the model using the predictions of the test set. We use accuracy and balanced accuracy. Here are their definitions:\n",
        "\n",
        "* **recall** in this context is also referred to as the true positive rate or sensitivity\n",
        "\n",
        "How many relevant item are selected\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "$${\\displaystyle {\\text{recall}}={\\frac {tp}{tp+fn}}\\,} $$\n",
        "\n",
        " \n",
        "\n",
        "* **specificity** true negative rate\n",
        "\n",
        "\n",
        "\n",
        "$${\\displaystyle {\\text{true negative rate}}={\\frac {tn}{tn+fp}}\\,}$$\n",
        "\n",
        "* **accuracy**: This measure gives you a sense of performance for all the classes together as follows:\n",
        "\n",
        "$$ {\\displaystyle {\\text{accuracy}}={\\frac {tp+tn}{tp+tn+fp+fn}}\\,}$$\n",
        "\n",
        "\n",
        "\\begin{equation*} accuracy=\\frac{number\\:of\\:correct\\:predictions}{(total\\:number\\:of\\:data\\:points (samples))} \\end{equation*}\n",
        "\n",
        "\n",
        "* **balanced accuracy**: This measure gives you a sense of performance for all the classes together as follows:\n",
        "\n",
        "$${\\displaystyle {\\text{balanced accuracy}}={\\frac {recall+specificity\n",
        "}{2}}\\,}$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdDOtXow1CKT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "5edc4c2e-3517-41c3-c17a-b68e16094051"
      },
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "print('Accuracy of the neural network model is:', metrics.accuracy_score(pred,test)*100)\n",
        "\n",
        "print(\"Blanced accuracy of the neural network model is:\", metrics.balanced_accuracy_score(pred, test))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the neural network model is: 97.74000000000001\n",
            "Blanced accuracy of the neural network model is: 0.9775660072665276\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}